Week 5 Problem Set
========================================================

Part I: A podcast!
-----------------

Listen to Planet Money's podcast Episode 677: The Experiment Experiment.
http://www.npr.org/sections/money/2016/01/15/463237871/episode-677-the-experiment-experiment

Part II: Adventures in multiple endpoints
-----------------

Hint: For many of these exercises, you may find it helpful to use R's dbinom(x,size,prob) function, where x corresponds to "z" (e.g., the number of heads; a vector from 0 to N) and size corresponds to N (the number of coin flips). R also has a function for the negative binomial, dnbinom(x,size,prob). Be careful if you use the negative binomial density, because the argument size corresponds to z (in the case of a negative binomial, this is a constant).

Suppose an experimenter plans to collect data on a coin-flipping experiment based on a two-tier stopping criterion (assume the coin is a fair coin). The experimenter will collect an initial batch of data with N=30 and then do a null hypothesis significance test. If the result is not significant, then an additional 15 subjects' data will be collected, for a total of 45. Suppose the researcher intends to use the standard critical values for determining significance at both the N=30 and N=45 stages. Our goal is to determine the actual false alarm rate (the Type I error rate $\alpha$) for this two-stage procedure, and to ponder what the mere intention of doing a second phase implies for interpreting the first stage, even if data collection stops with the first stage.

(A; 4 pts) For N=30, what are the lower ($z_{low}$) and upper ($z_{high}$) limits of the 95th percentile confidence interval for z (z=number of heads)

$$
p(z \leq z_{low} \vert N=30,\theta=0.5)<0.025 \\
p(z \geq z_{high} \vert N=30,\theta=0.5)<0.025
$$

assuming a two-tailed Type I error rate of 0.05 or less?
(COPY AND PASTE ANSWER AND ANY CODE BELOW)

```{r}
rm(list=ls(all=TRUE))
N=30 #number of trials
alpha.2<-0.025#lower tail critical value
pb<-.5 #probability of heads or tails (success or failure for a fair coin)
#significance 
X.ql<-qbinom(alpha.2, size = N, prob = pb)#find the quantile for alpha/2
X.ql<-X.ql-1 #I subtract by 1 to get a lower bound for a discrete distribution, less and not equal to the lower critical value
r<-pbinom(N-X.ql, 30,.5)#subtract lower bound from number of trials to get upper bound, because of symmetry
X.qu<-qbinom(p = r, size = 30, prob = .5)#upper bound limit
cat("The lower and upper bounds for the 95 percent confidence interval are", X.ql,"and",X.qu)


```


(B; 4 pts) For N=45, what are the lower ($z_{low}$) and upper ($z_{high}$) limits of the 95th percentile confidence interval for z (z=number of heads)

$$
p(z\leq z_{low}\vert N=45,\theta=0.5)<0.025 \\
p(z\geq z_{high}\vert N=45,\theta=0.5)<0.025
$$

assuming a two-tailed Type I error rate of 0.05 or less?
(COPY AND PASTE ANSWER AND ANY CODE BELOW)

```{r}
N.b=45 #number of trials
alpha.d2<-0.025#upper tail critical value
pb.b<-.5 #probability of heads or tails (success or failure for a fair coin)
#significance 
X.ql.b<-qbinom(alpha.d2, size = N.b, prob = pb.b)#find the quantile for alpha/2
X.ql.b<-X.ql.b-1##I subtract by 1 to get a lower bound for a discrete distribution, less and not equal to the lower critical value
X.ql.u<-qbinom(p = .975, size = 45, prob = .5)
X.ql.u<-X.ql.u+1#I add 1 to get the upper bound for a discrete distribution, greater and not equal to the upper critical value
cat("The lower and upper bounds for the 95 percent confidence interval are", X.ql.b,"and",X.ql.u)

```

For the next part of the exercise, consider the table provided. Each cell of the table corresponds to a certain outcome from the first 30 flips of a fair coin and a certain outcome from the second 15 flips of the same fair coin. A cell is marked by a dagger, $\maltese$, if it has a result for the first 30 flips that would reject the null hypothesis. A cell is marked by a star, $\bigstar$, if it has a result for the total of 45 flips that would reject the null hypothesis. For example, the cell with 10 heads from the first 30 flips and 1 head from the second 15 flips is marked with a $\bigstar$ because the total number of heads for that cell, 10+1=11, is less than 15 (which is z_low for N=45 [a hint for part B!]). That cell has no dagger, $\maltese$, because getting 10 heads in the first 30 flips is not extreme enough to reject the null. If neither the first 30 coin flips, nor the second 15 coin flips, would reject the null hypothesis of a fair coin, than the cell is marked with a dash -.
 
(C; 3 pts) Denote the number of heads in the first 30 flips as $z_{1}$, and the number of heads in the second 15 flips as $z_{2}$. Explain why it is true that the $z_{1}$,$z_{2}$ cell of the table has a joint probability equal to dbinom(z1,30,0.5)*dbinom(z2,15,0.5).

dbinom(z1,30,.5) generates the sum of a discrete probability distribution describring the outcome of n=30 independent trials at and before z1 heads, or the sum of the probability of obtaining z1 heads.
dbinom(z1,30,.5) generates the sum of a discrete probability distribution describing the outcome of n=15 independent trials at and before z2 heads, or the sum of the probability of obtaining z2 heads.

Since the probability of obtaining $Z_{1}$ heads in the first 30 flip does not affect the outcome $z_{2}$ heads in the second 15 flips, we define these events as independent. The joint probability of the two independent draws $z_{1}$ and $z_{2}$ is the product of the sum of each of their independent probablities. 
$$P(Z_1 \cap  Z_2)= 1.0= P(Z_1)*P(Z_2)$$




(D; 4 pts) What is the sum of the probabilities of all the cells that contain a $\maltese$ (whether or not it contains a $\bigstar$)? Explain how you got your answer! 

I computed the sum of the probabilities of cells that contains daggers up to z1 my lower bound, which are the outcome probabilities within the distribution that would reject the null hypothesis. The dbinom function computed this. The type one error probability density for the lower tail, or cells that contain a dagger on the lower tail, is defined in the interval 0-9, and therefore my command is sum(dbinom(0:9,30,.5)). Because this area on the lower tail is proportional to the upper tail significance area, I multiply by two to get the type 1 false alarm error rate for a distribution of 30 flips, the sum of the probabilities of all the cells that contain a dagger.
 
```{r}

areas.w.dagger<-2*sum(dbinom(0:9,30,.5))
cat("The sum of the probabilities of all the cells that contain dagger:" ,areas.w.dagger)
```



(E; 4 pts) What is the sum of the probabilities of all the cells that contain a $\bigstar$ (whether or not it contains a $\maltese$)? Explain how you got your answer! 

I computed the sum of the probabilities of cells that contain bigstars at z1 my lower bound. The dbinom function computed this. The cells that contain a big star in the lower tail, is defined in the interval 0-15, and therefore my command is sum(pbinom(0:15,45,.5)), which computes the cumulative distribution of the probabilities. The pbinom function offers an option to calculate the CDF of the upper tail, which I use with the upper bound for the 45 flips to get the sum of the probabilities of all the cells that contain a bigstar.

```{r}

l.bigstar<-pbinom(15, 45, 0.5, lower.tail = T)
l.bigstar<-pbinom(29, 45, 0.5, lower.tail = F)
areas.w.bigstar<-sum(l.bigstar)+sum(l.bigstar)
cat("The sum of the probabilities of all the cells that contain big star:" ,areas.w.bigstar)

```



(F; 4 pts) What is the sum of the probabilities of all the cells that contain either a $\maltese$ or a $\bigstar$? (Note: This is the Type I error rate for the two-stage design, because these are all the ways you would decide to reject the null even when it is true.) Explain how you got your answer! 

To compute this I computed the probabilities for the areas that have a dash in the graph, that is areas that did not have a dagger or big star for a two-stage design of 45 flips and are within the confidence interval for the null hypothesis probability density. These areas include the range from 16 to 29, which are between the discrete interval of 15 and 30 for the increased type I error rate for the two-stage design of 45 flips. The difference between the total probability density (1), and the sum of the probabilities of these dash cells which do not contain a dagger or big star yields the sum of the probabilities of all cells that contain a dagger and/or big star.

```{r}
sample.space<- c()
for (null in 10:20){ 
  for (k2 in 0:15){ 
      if (16 <=(null+k2) & (null+k2)<= 29){ 
      sample.space<-append(sample.space, (dbinom(x = null, size = 30, prob = .5) * dbinom(x = k2 , size = 15, prob = .5)))
      }
  }
}
alpha.45<-1-sum(sample.space)
cat("The sum of the probabilities of all cell that contain a dagger and/or big star is",alpha.45 )

```

G; 7 pts Suppose that the researcher intends to run an experiment using this two-stage stopping criterion. She collects the first 30 flips and finds 8 heads. She therefore rejects the null hypothesis and reports that p<0.05. Is that correct? Explain.

She is not correct. In a two-stage stopping criteria the threshold for significance difference from the null hypothesis has changed from the 9 heads threshold for the one-stage experiment of 30 flips to the zlow value 15 for 45 flips and she would be not correctly rejecting the null hypothesis. In this two-stage stopping experiment the type I error rate is inflated and no longer <.05. She should report the actual inflated type one error rate.
```{r}
cat ("In a two-stage experiment the actual type I error rate is",alpha.45)

```


(H; 5 points BONUS) Whenever we run an experiment and get a result that trends away from the null experiment, but isn't quite significant, it's natural to consider collecting more data. We saw in the previous part that even intending to collect more data, but not actually doing it, inflates the Type I error rate. 
Doesn't the fact that we always consider collecting more data mean that we always have a much higher Type I error rate than we pretend we do? 
Doesn't the actual Type I error rate of an experiment depend on the maximal number of data points we'd be willing to collect over the course of our lifetimes? 
In 1-2 paragraphs, discuss this conundrum and decide whether or not you think this poses a fundamental problem with null hypothesis testing. 

WHen collecting more data for an experiment the experimenter after a predetermined alpha is likely be faced with an inflated type I error rate. To mitigate this, the Type 1 error rate should be fixed a priori considering the sample size ahead of time when designing an experiment. If the results do not allow for the null hypothesis to be rejected, the experimenter can use a post hoc power analysis to assess whether the experiment should be completed again with a sample size larger than was tested previously.
The actual Type I error rate of an experiment does depend on the maximal number of data points we'd be willing to collect over the course of our lifetimes. With enough data we are able to reject the null hypothesis purely by chance just as it was commented on in the podcast.
Inflated type I errors with increased sample sizes and multiple endpoints does pose a fundamental problem with null hypothesis testing because the p-value is the "Gold standard" used in the scientific community as a means to assess whether or not there was an effect. This arbitrary "Gold standard" if not computed corectly, that is determing sample size and the true type I error rate a priori for instance, allows the possibility of a false refute of the null hypothesis and undermines the published data that other scientists are likely to base their research on.

  



